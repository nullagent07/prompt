version: "3.9"

# Single-node inference stack (LLM + Embeddings + Reranker) + GPU/host monitoring.
# Qdrant is NOT included (runs on another machine).
#
# IMPORTANT (Astra/parsec):
# - If GPU access works only with --privileged on your host, keep `privileged: true` on GPU services for MVP.
# - Once Astra parsec policy is fixed (allow /dev/nvidia* to dockerd), set `privileged: false`.
#
# Exposes:
# - LLM (vLLM OpenAI-compatible):        http://localhost:8000/v1/...
# - Embeddings (TEI):                    http://localhost:8081/...
# - Rerank (TEI):                        http://localhost:8082/rerank
# - Prometheus:                          http://localhost:9090
# - Grafana:                             http://localhost:3000
# - DCGM Exporter (GPU metrics):         http://localhost:9400/metrics
#
# Minimal env you likely need:
# - HF_TOKEN (if any model is gated/private)
# - Optional proxy: HTTP_PROXY / HTTPS_PROXY / NO_PROXY

services:
  # ----------------------------
  # LLM Inference (vLLM)
  # ----------------------------
  vllm_llm:
    image: vllm/vllm-openai:latest
    container_name: vllm_llm
    restart: unless-stopped
    privileged: true  # <-- set to false after Astra/parsec device policy is fixed
    environment:
      # Model to serve (pick one that fits your VRAM/latency goals)
      # Examples (replace as needed):
      #   - Qwen/Qwen2.5-32B-Instruct
      #   - meta-llama/Llama-3.1-70B-Instruct (very heavy)
      #   - deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
      MODEL_ID: ${LLM_MODEL_ID:-Qwen/Qwen2.5-32B-Instruct}

      # HuggingFace auth + (optional) proxy
      HF_TOKEN: ${HF_TOKEN:-}
      HTTP_PROXY: ${HTTP_PROXY:-}
      HTTPS_PROXY: ${HTTPS_PROXY:-}
      NO_PROXY: ${NO_PROXY:-localhost,127.0.0.1,::1}

      # vLLM tuning
      VLLM_LOGGING_LEVEL: ${VLLM_LOGGING_LEVEL:-INFO}
    ports:
      - "8000:8000"
    volumes:
      - hf_cache:/root/.cache/huggingface
      - vllm_cache:/root/.cache/vllm
    command:
      - "--model"
      - "${LLM_MODEL_ID:-Qwen/Qwen2.5-32B-Instruct}"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
      - "--served-model-name"
      - "${LLM_SERVED_NAME:-llm}"
      - "--dtype"
      - "${LLM_DTYPE:-auto}"            # auto/bfloat16/float16
      - "--gpu-memory-utilization"
      - "${LLM_GPU_MEM_UTIL:-0.90}"     # 0.80-0.95 typical
      - "--max-model-len"
      - "${LLM_MAX_LEN:-8192}"
      - "--enforce-eager"
      - "${LLM_ENFORCE_EAGER:-false}"   # set true if you hit weird kernel/runtime issues
      - "--enable-prefix-caching"
      - "${LLM_PREFIX_CACHE:-true}"
      - "--swap-space"
      - "${LLM_SWAP_GB:-8}"             # helps avoid OOM spikes (uses host RAM)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:8000/health || exit 1"]
      interval: 20s
      timeout: 5s
      retries: 20

  # ----------------------------
  # Embeddings (TEI)
  # ----------------------------
  tei_embeddings:
    image: ghcr.io/huggingface/text-embeddings-inference:latest
    container_name: tei_embeddings
    restart: unless-stopped
    privileged: true  # <-- set to false after Astra/parsec device policy is fixed
    environment:
      HF_TOKEN: ${HF_TOKEN:-}
      HTTP_PROXY: ${HTTP_PROXY:-}
      HTTPS_PROXY: ${HTTPS_PROXY:-}
      NO_PROXY: ${NO_PROXY:-localhost,127.0.0.1,::1}
    ports:
      - "8081:80"
    volumes:
      - hf_cache:/data/.cache/huggingface
    command:
      - "--model-id"
      - "${EMB_MODEL_ID:-BAAI/bge-m3}"  # strong multilingual embeddings (RU ok)
      - "--dtype"
      - "${EMB_DTYPE:-float16}"
      - "--max-batch-tokens"
      - "${EMB_MAX_BATCH_TOKENS:-8192}"
      - "--max-concurrent-requests"
      - "${EMB_MAX_CONCURRENCY:-64}"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:80/health || exit 1"]
      interval: 20s
      timeout: 5s
      retries: 20

  # ----------------------------
  # Reranker (TEI rerank)
  # ----------------------------
  tei_rerank:
    image: ghcr.io/huggingface/text-embeddings-inference:latest
    container_name: tei_rerank
    restart: unless-stopped
    privileged: true  # <-- set to false after Astra/parsec device policy is fixed
    environment:
      HF_TOKEN: ${HF_TOKEN:-}
      HTTP_PROXY: ${HTTP_PROXY:-}
      HTTPS_PROXY: ${HTTPS_PROXY:-}
      NO_PROXY: ${NO_PROXY:-localhost,127.0.0.1,::1}
    ports:
      - "8082:80"
    volumes:
      - hf_cache:/data/.cache/huggingface
    command:
      - "--model-id"
      - "${RERANK_MODEL_ID:-BAAI/bge-reranker-v2-m3}"  # multilingual reranker (RU ok)
      - "--task"
      - "rerank"
      - "--dtype"
      - "${RERANK_DTYPE:-float16}"
      - "--max-batch-tokens"
      - "${RERANK_MAX_BATCH_TOKENS:-4096}"
      - "--max-concurrent-requests"
      - "${RERANK_MAX_CONCURRENCY:-32}"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:80/health || exit 1"]
      interval: 20s
      timeout: 5s
      retries: 20

  # ----------------------------
  # GPU telemetry exporter (Prometheus scrape target)
  # ----------------------------
  dcgm_exporter:
    image: nvcr.io/nvidia/k8s/dcgm-exporter:4.5.2-4.8.1-distroless
    container_name: dcgm_exporter
    restart: unless-stopped
    privileged: true  # read-only GPU telemetry sometimes needs extra privileges on hardened hosts
    cap_add:
      - SYS_ADMIN
    ports:
      - "9400:9400"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Host metrics (CPU/RAM/Disk/Net)
  node_exporter:
    image: prom/node-exporter:latest
    container_name: node_exporter
    restart: unless-stopped
    pid: host
    network_mode: host
    command:
      - "--path.rootfs=/host"
    volumes:
      - "/:/host:ro,rslave"

  # Container metrics (optional but useful)
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cadvisor
    restart: unless-stopped
    privileged: true
    ports:
      - "8089:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro

  # ----------------------------
  # Prometheus (config generated at runtime into a named volume)
  # ----------------------------
  prometheus_config_init:
    image: alpine:3.20
    container_name: prometheus_config_init
    restart: "no"
    volumes:
      - prometheus_config:/etc/prometheus
    entrypoint: ["/bin/sh","-lc"]
    command: |
      cat > /etc/prometheus/prometheus.yml <<'EOF'
      global:
        scrape_interval: 15s
        evaluation_interval: 15s

      scrape_configs:
        - job_name: "dcgm_exporter"
          static_configs:
            - targets: ["dcgm_exporter:9400"]

        - job_name: "vllm_llm"
          metrics_path: "/metrics"
          static_configs:
            - targets: ["vllm_llm:8000"]

        - job_name: "tei_embeddings"
          metrics_path: "/metrics"
          static_configs:
            - targets: ["tei_embeddings:80"]

        - job_name: "tei_rerank"
          metrics_path: "/metrics"
          static_configs:
            - targets: ["tei_rerank:80"]

        - job_name: "cadvisor"
          static_configs:
            - targets: ["cadvisor:8080"]

        - job_name: "node_exporter"
          static_configs:
            - targets: ["node_exporter:9100"]
      EOF
      echo "prometheus.yml generated."

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    depends_on:
      - prometheus_config_init
      - dcgm_exporter
      - node_exporter
      - cadvisor
      - vllm_llm
      - tei_embeddings
      - tei_rerank
    ports:
      - "9090:9090"
    volumes:
      - prometheus_config:/etc/prometheus
      - prometheus_data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=${PROM_RETENTION:-15d}"
      - "--web.enable-lifecycle"

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    depends_on:
      - prometheus
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASS:-admin}
      GF_USERS_ALLOW_SIGN_UP: "false"
    volumes:
      - grafana_data:/var/lib/grafana

volumes:
  hf_cache:
  vllm_cache:
  prometheus_config:
  prometheus_data:
  grafana_data:
